from tokenizers.pre_tokenizers import BertPreTokenizer
from get_vocab import get_vocabulary
from heap import MaxHeap
import os,sys
from collections import Counter, defaultdict
import numpy as np
from tqdm import tqdm
import json
import argparse
import copy
import re
import pickle
from scipy.special import softmax
import numpy as np
from scipy.stats import hmean
from scipy.special import logsumexp

def softmin_lin(f1, f2, tau):
  f1_exp = np.exp(-tau*f1)
  f2_exp = np.exp(-tau*f2)
  ratio = f1_exp/(f1_exp+f2_exp)
  return (f1*ratio + f2*(1-ratio))

def softmin_log(f1, f2, tau):
  f1_exp = np.exp(-tau*f1)
  f2_exp = np.exp(-tau*f2)
  return (np.log(f1_exp+f2_exp))/tau

class MultilingualTokenizer(object):
    """Base Class for the Multilingual Tokenizer"""

    def __init__(self,input_files,vocab_files,output_dir,vocab_size,n_HRL,overlap="product",p=-2,use_vocab=True,eow_suffix="",cw_prefix="",num_workers=1,alpha=0.5,tau=0.001):
        """Initializer for the tokenizer
        Args:
            input_files (list of str): List of input files for the tokenizer, for the different monolingual data
            vocab_files (list of str): List of vocab files generated by get_vocab.py
            output_dir (str): Path to output directory
            vocab_size (int): Size of the vocabulary generated
            use_vocab (bool): Whether to use vocab files or text files. Defaults to True
            eow_suffix (str, optional): End of word suffix (for BPE). Defaults to "".
            cw_prefix (str, optional): Continuing word prefix (for WordPiece). Defaults to "".
            num_workers (int,optional): Number of workers for getting the initial vocabulary. Defaults to 1.
            alpha (float,optional): Weight given to average in tokenization maximization algorithm
        """
        self.input_files    = input_files
        self.vocab_files    = vocab_files
        self.use_vocab      = use_vocab
        self.n_lang         = len(self.input_files) if self.input_files is not None else len(self.vocab_files)
        self.n_HRL          = n_HRL                    
        self.num_sentences  = np.zeros(self.n_lang,dtype=int)
        self.output_dir     = output_dir
        self.vocab_size     = vocab_size
        self.eow_suffix     = eow_suffix
        self.cw_prefix      = cw_prefix
        self.final_vocab    = []
        self.merges         = []
        self.stats          = defaultdict(lambda: np.zeros(self.n_lang,dtype=int)) #Dict of np.ndarray
        self.cost_stats     = {}
        self.inv_cost_stats = defaultdict(set) #Dict of set
        self.cost_heap      = MaxHeap()
        self.ind_vocabs     = [] #List of lists
        self.tok_counts     = defaultdict(lambda: np.zeros(self.n_lang,dtype=int)) #List of Counters
        self.indices        = [defaultdict(lambda: defaultdict(int)) for _ in range(self.n_lang)] #List of dict
        self.num_workers    = num_workers
        self.alpha          = alpha
        self.tau            = tau
        self.overlap        = overlap
        self.p              = p

        os.makedirs(output_dir,exist_ok=True)

    def cost_function(self,pair):
        """Get the cost function of the given pair. Needs to be implemented by the child class
        Args:
            pair (tuple): pair for which we want the function
        Returns:
            float: Cost function of the pair
        """
        raise NotImplementedError


    def update_stats(self,pair,index,value):
        """Update the stats along with the other variables for the given pair
        Args:
            pair (tupe): Pair for which update needs to be made
            indices (int): Index for which the changes need to be made
            values (int): New value for the given indices
        """
        if pair not in self.stats:
            assert (pair not in self.cost_stats)

        self.stats[pair][index] += value

        new_cost = self.cost_function(pair)

        if pair in self.cost_stats:
            prev_cost = self.cost_stats[pair]
        else:
            prev_cost = None

        self.cost_stats[pair] = new_cost
        if prev_cost is not None:
            assert (prev_cost in self.inv_cost_stats)

            if prev_cost == new_cost:
                return

            self.inv_cost_stats[prev_cost].remove(pair)
            if not self.inv_cost_stats[prev_cost]:
                self.cost_heap.delete(prev_cost)

        if not self.inv_cost_stats[new_cost]:
            self.cost_heap.push(new_cost)

        self.inv_cost_stats[new_cost].add(pair)

    def delete_pair(self,pair):
        """Delete the pair from the stats table and update the required variables
        Args:
            pair (tuple): Pair to be deleted
        """
        assert (pair in self.stats and pair in self.cost_stats)
        prev_cost = self.cost_stats[pair]
        self.stats.pop(pair)
        self.cost_stats.pop(pair)
        assert (prev_cost in self.inv_cost_stats)
        assert (pair not in self.inv_cost_stats[prev_cost]) #Since the pair has already been popped from the set while getting the maximum pair
        if not self.inv_cost_stats[prev_cost]:
            self.cost_heap.delete(prev_cost)
            self.inv_cost_stats.pop(prev_cost)

    def process_vocab(self,vocabs):
        """Process the vocabulary obtained from the monolingual data (Splitting the words and adding end of suffix etc.)
        Args:
            vocabs (List of Counters): List of vocab files obatined from the monolingual data
        """

        print("Processing the vocabulary")
        original_chars = set()
        processed_chars = set()
        for i,vocab in enumerate(vocabs):
            print("Input file:",i)
            vocab_dict = {}
            for word,freq in tqdm(vocab.items()):
                new_word = []
                for j,x in enumerate(word):
                    original_chars.add(x)
                    x_ = x
                    if j!=0:
                        x_ = self.cw_prefix+x_
                    if j==len(word)-1:
                        x_ = x_ + self.eow_suffix
                    if x_!=x:
                        processed_chars.add(x_)
                    new_word.append(x_)
                    self.tok_counts[x_][i]+=freq
                vocab_dict[tuple(new_word)] = freq
            self.ind_vocabs.append(sorted(vocab_dict.items(),key=lambda x: x[1], reverse=True))

        self.final_vocab.extend(list(original_chars))
        self.final_vocab.extend(list(processed_chars))

    def get_pair_statistics(self):
        """Count frequency of all symbol pairs and initialize the statistics"""

        print("Getting pairwise statistics")
        for i,vocab in enumerate(self.ind_vocabs):
            print("Input file:",i)
            for j, (word, freq) in tqdm(enumerate(vocab)):
                prev_char = word[0]
                for char in word[1:]:
                    self.update_stats((prev_char,char),i,freq)
                    self.indices[i][(prev_char, char)][j] += 1
                    prev_char = char

    def replace_pair(self,pair):
        """Replace all occurrences of a symbol pair ('A', 'B') with a new symbol 'AB'
        Args:
            pair (tuple): Pair to be replaced
        Returns:
            tuple: Changes to be reflected in statistics
        """
        if self.cw_prefix:
            assert (pair[1][:len(self.cw_prefix)] == self.cw_prefix)
            if (pair[0][:len(self.cw_prefix)] == self.cw_prefix):
                pair_str = self.cw_prefix + pair[0][len(self.cw_prefix):] + pair[1][len(self.cw_prefix):]
            else:
                pair_str = pair[0] + pair[1][len(self.cw_prefix):]

        else:
            pair_str = ''.join(pair)

        pattern = re.compile(r'(?<!\S)' + re.escape(pair[0] + ' ' + pair[1]) + r'(?!\S)')
        changes = []
        for i in range(self.n_lang):
            iterator = self.indices[i][pair].items()
            for j, freq in iterator:
                if freq < 1:
                    continue
                word, freq = self.ind_vocabs[i][j]
                new_word = ' '.join(word)
                new_word = pattern.sub(pair_str, new_word)
                new_word = tuple(new_word.split(' '))
                self.ind_vocabs[i][j] = (new_word, freq)
                self.tok_counts[pair[0]][i] -= freq
                self.tok_counts[pair[1]][i] -= freq
                self.tok_counts[pair_str][i] += freq
                changes.append((i, j, new_word, word, freq))

        return changes, pair_str

    def update_pair_statistics(self, pair, new_pair, changes):
        """Minimally update the indices and statistics; if we merge a pair of symbols,
       only pairs that overlap with occurrences of this pair are affected, and need to be updated.
        Args:
            pair (tuple): Pair to be merged
            new_pair (str): Merged pair to be used in place of the original pair
            changes (list of tuple): Changes made in the vocab file
        """
        self.delete_pair(pair)
        for i in range(self.n_lang):
            self.indices[i][pair] = defaultdict(int)
        first, second = pair
        for ind, j, word, old_word, freq in changes:

            # find all instances of pair, and update frequency/indices around it
            i = 0
            while True:
                # find first symbol
                try:
                    i = old_word.index(first, i)
                except ValueError:
                   break
                # if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])
                if i < len(old_word)-1 and old_word[i+1] == second:
                    # assuming a symbol sequence "A B C", if "B C" is merged, reduce the frequency of "A B"
                    if i:
                        prev = tuple(old_word[i-1:i+1])
                        self.update_stats(prev,ind,-freq)
                        self.indices[ind][prev][j] -= 1
                    if i < len(old_word)-2:
                        # assuming a symbol sequence "A B C B", if "B C" is merged, reduce the frequency of "C B".
                        # however, skip this if the sequence is A B C B C, because the frequency of "C B" will be reduced by the previous code block
                        if old_word[i+2] != first or i >= len(old_word)-3 or old_word[i+3] != second:
                            nex = tuple(old_word[i+1:i+3])
                            self.update_stats(nex,ind,-freq)
                            self.indices[ind][nex][j] -= 1
                    i += 2
                else:
                    i += 1

            i = 0
            while True:
                try:
                    # find new pair
                    i = word.index(new_pair, i)
                except ValueError:
                    break
                # assuming a symbol sequence "A BC D", if "B C" is merged, increase the frequency of "A BC"
                if i:
                    prev = tuple(word[i-1:i+1])
                    self.update_stats(prev,ind,freq)
                    self.indices[ind][prev][j] += 1
                # assuming a symbol sequence "A BC B", if "B C" is merged, increase the frequency of "BC B"
                # however, if the sequence is A BC BC, skip this step because the count of "BC BC" will be incremented by the previous code block
                if i < len(word)-1 and word[i+1] != new_pair:
                    nex = tuple(word[i:i+2])
                    self.update_stats(nex,ind,freq)
                    self.indices[ind][nex][j] += 1
                i += 1

    def train(self):
        """Learn the tokenization from the input files
        """
        #Get the vocabulary from the monolingual data
        vocabs = []
        if not self.use_vocab:
            pre_tokenizer = BertPreTokenizer()
            print("Getting the vocabulary from input files")
            print(self.use_vocab)
            for i,filename in enumerate(self.input_files):
                print(filename)
                with open(filename) as f:
                    vocab,sentences = get_vocabulary(f,pre_tokenizer,self.num_workers)
                    vocabs.append(vocab)
                    self.num_sentences[i] = sentences

        else:
            print("Getting vocabulary from vocab files")
            for i,filename in enumerate(self.vocab_files):
                print(filename)
                with open(filename,'rb') as f:
                    vocab,sentences = pickle.load(f)
                    vocabs.append(vocab)
                    self.num_sentences[i] = sentences

        #Process the vocab
        self.process_vocab(vocabs)

        #Get pairwise statistics
        self.get_pair_statistics()

        char_vocab = len(self.final_vocab)
        print("Character vocabulary size:",char_vocab)
        num_merges = self.vocab_size-char_vocab
        print("Number of merges required for %d vocab size: %d"%(self.vocab_size,num_merges))

        #Merge loop
        for m in tqdm(range(num_merges)):
            max_occurence = self.cost_heap.max()
            if max_occurence == -1:
                print("No more merges possible. Exiting loop")
                break

            assert (max_occurence in self.inv_cost_stats)
            pair = self.inv_cost_stats[max_occurence].pop()

            #Replace the existing pair with the merged pair
            changes,new_pair = self.replace_pair(pair)

            #Update the statistics table
            self.update_pair_statistics(pair,new_pair,changes)

            assert("".join(pair)==new_pair)
            self.final_vocab.append(new_pair)
            self.merges.append(pair)

    def save(self):
        out_vocab = os.path.join(self.output_dir,'vocab.json')
        out_merges = os.path.join(self.output_dir,'merges.txt')
        vocab_json = dict([("[PAD]",0)]+[(vocab,i+1) for i,vocab in enumerate(self.final_vocab)])
        with open(out_vocab,'w',encoding='utf-8') as w:
            json.dump(vocab_json,w,ensure_ascii=False,indent=4)
        with open(out_merges,'w') as w:
            w.write("#version: 0.2\n")
            for merge in self.merges:
                w.write(merge[0]+" "+merge[1]+"\n")



class BPEMultiTokenizer(MultilingualTokenizer):

    def cost_function(self,pair):
        if self.overlap=="min":
          incremental_prob = np.sum([np.max([np.min([self.stats[pair][h] , self.stats[pair][k]]) for h in range(self.n_HRL)]) for k in range(self.n_HRL,self.n_lang)])
        elif self.overlap=="product":
          incremental_prob_prod = np.sum([np.max([self.stats[pair][h]*self.stats[pair][k] for h in range(self.n_HRL)]) for k in range(self.n_HRL,self.n_lang)])
          incremental_prob = incremental_prob_prod*10e-6
        elif self.overlap=="minprod":
          incremental_prob_min = np.sum([np.max([np.min([self.stats[pair][h] , self.stats[pair][k]]) for h in range(self.n_HRL)]) for k in range(self.n_HRL,self.n_lang)])
          incremental_prob_prod = np.sum([np.max([self.stats[pair][h]*self.stats[pair][k] for h in range(self.n_HRL)]) for k in range(self.n_HRL,self.n_lang)])
          incremental_prob = incremental_prob_min + incremental_prob_prod*10e-6
        elif self.overlap=="softmin_log":
          incremental_prob = np.sum([np.max([softmin_log(self.stats[pair][h] , self.stats[pair][k], self.tau) for h in range(self.n_HRL)]) for k in range(self.n_HRL,self.n_lang)])
        elif self.overlap=="softmin_lin":
          incremental_prob = np.sum([np.max([softmin_lin(self.stats[pair][h] , self.stats[pair][k], self.tau) for h in range(self.n_HRL)]) for k in range(self.n_HRL,self.n_lang)])
        elif self.overlap=="geom":
          incremental_prob = np.sum([np.max([np.sqrt(self.stats[pair][h]*self.stats[pair][k]) for h in range(self.n_HRL)]) for k in range(self.n_HRL,self.n_lang)])
        elif self.overlap=="hm":
          incremental_prob = np.sum([np.max([hmean([self.stats[pair][h],self.stats[pair][k]]) if (self.stats[pair][h]>0 and self.stats[pair][k]>0) else 0 for h in range(self.n_HRL)]) for k in range(self.n_HRL,self.n_lang)])
        elif self.overlap=="mean":  
          return self.alpha*np.sum(self.stats[pair]) + (1-self.alpha)*(np.sum([np.max([np.exp(logsumexp([self.p*np.log(self.stats[pair][h]), self.p*np.log(self.stats[pair][k])])/self.p)/2**(1/self.p) if (self.stats[pair][h]>0 and self.stats[pair][k]>0) else 0 for h in range(self.n_HRL)]) for k in range(self.n_HRL,self.n_lang)]))  
        else:
          print("INVALID OVERLAP")
        return self.alpha*np.sum(self.stats[pair]) + (1-self.alpha)*incremental_prob

class BPETokenizer(MultilingualTokenizer):

    def cost_function(self,pair):
        return np.sum(self.stats[pair])


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--mono_files',type=str,nargs='+',help='Path to monolingual text files')
    parser.add_argument('--vocab_files',type=str,nargs='+',help='Path to vocabulary files generated from get_vocab.py')
    parser.add_argument('--use_vocab',action='store_true',help='Use the vocab files instead of the text files')
    parser.add_argument('--output_dir',type=str,help='Path to the output directory where models are stored')
    parser.add_argument('--vocab_size',type=int,help='Size of the vocabulary file')
    parser.add_argument('--model',type=str,help='Model type for tokenization',choices=['bpe','wordpiece','sentencepiece'])
    parser.add_argument('--max_tok',action='store_true',help='Modify the tokenization algorithm to maximize overlap')
    parser.add_argument('--eow_suffix',type=str,default='',help='End of word suffix (e.g. BPE algorithm)')
    parser.add_argument('--cw_prefix',type=str,default='',help='Continuing word prefix (e.g. WordPiece Algorithm)')
    parser.add_argument('--num_workers',type=int,default=1,help='Number of workers for building vocabulary from mono data')
    parser.add_argument('--alpha',type=float,default=0.5,help='Weight given to Average for token overlap maximization algorithm')
    parser.add_argument('--tau',type=float,default=0.001,help='Temperature parameter of softmin')
    parser.add_argument('--n_HRL',type=int,default=1,help='number of HRLs')
    parser.add_argument('--overlap',type=str,help='Type of overlap',choices=['min','product','softmin_log','softmin_lin','minprod','geom','hm','mean'])
    parser.add_argument('--p',type=int,default=-2,help='hyperparameter in generalized mean')

    args = parser.parse_args()

    if args.model == 'bpe':
        if args.max_tok:
            tok_class = BPEMultiTokenizer
        else:
            tok_class = BPETokenizer

    tokenizer = tok_class(args.mono_files,args.vocab_files,args.output_dir,args.vocab_size,args.n_HRL,args.overlap,args.p,args.use_vocab,args.eow_suffix,args.cw_prefix,args.num_workers,args.alpha,args.tau)
    tokenizer.train()
    tokenizer.save()
        
